Consider the estimation of a signal {\bf x}\in\mathbb{R}^N from noisy observations {\bf r=x+z}, where the input~{\bf x} is generated by an independent and identically distributed (i.i.d.) Gaussian mixture source, and {\bf z} is additive white Gaussian noise (AWGN) in parallel Gaussian channels. Typically, the \ell_2-norm error (squared error) is used to quantify the performance of the estimation process. In contrast, we consider the \ell_\infty-norm error (worst case error). For this error metric, we prove that, in an asymptotic setting where the signal dimension N\to\infty, the \ell_\infty-norm error always comes from the Gaussian component that has the largest variance, and the Wiener filter asymptotically achieves the optimal expected \ell_\infty-norm error. The i.i.d. Gaussian mixture case is easily applicable to i.i.d. Bernoulli-Gaussian distributions, which are often used to model sparse signals. Finally, our results can be extended to linear mixing systems with i.i.d. Gaussian mixture inputs, in settings where a linear mixing system can be decoupled to parallel Gaussian channels.