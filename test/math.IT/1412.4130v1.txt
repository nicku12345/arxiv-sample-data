Thompson's model of VLSI computation relates the energy of a computation to the product of the circuit area and the number of clock cycles needed to carry out the computation. It is shown that for any family of circuits implemented according to this model, using any algorithm that performs decoding of a codeword passed through a binary erasure channel, as the block length approaches infinity either (a) the probability of block error is asymptotically lower bounded by 1/2 or (b) the energy of the computation scales at least as Omega(n(log n)^(1/2)), and so the energy of successful decoding, per decoded bit, must scale at least as Omega((log n)^(1/2)). This implies that the average energy per decoded bit must approach infinity for any sequence of codes that approaches capacity. The analysis techniques used are then extended to the case of serial computation, showing that if a circuit is restricted to serial computation, then as block length approaches infinity, either the block error probability is lower bounded by 1/2 or the energy scales at least as fast as Omega(n log(n)). In a very general case that allows for the number of output pins to vary with block length, it is shown that the average energy per decoded bit must scale as Omega(n(log n)^(1/5)). A simple example is provided of a class of circuits performing low-density parity-check decoding whose energy complexity scales as O(n^2 log log n).