We present the convergence rates and the explicit error bounds of Hill's method, which is a numerical method for computing the spectra of ordinary differential operators with periodic coefficients. This method approximates the operator by a finite dimensional matrix. On the assumption that the operator is selfadjoint, it is shown that, under some conditions, we can obtain the convergence rates of eigenvalues with respect to the dimension and the explicit error bounds. Numerical examples demonstrate that we can verify these conditions using Gershgorin's theorem for some real problems. Main theorems are proved using the Dunford integrals which project an eigenvector to the corresponding eigenspace.