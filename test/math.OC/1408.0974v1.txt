We study a forward backward splitting algorithm that solves the variational inequality \begin{equation*} A x +\nabla \Phi(x)+ N_C (x) \ni 0 \end{equation*} where H is a real Hilbert space, A: H\rightrightarrows H is a maximal monotone operator, \Phi: H\to\mathbb{R} is a smooth convex function, and N_C is the outward normal cone to a closed convex set C\subset H. The constraint set C is represented as the intersection of the sets of minima of two convex penalization function \Psi_1:H\to\mathbb{R} and \Psi_2: H\to\mathbb{R}\cup \{+\infty\}. The function \Psi_1 is smooth, the function \Psi_2 is proper and lower semicontinuous. Given a sequence (\beta_n) of penalization parameters which tends to infinity, and a sequence of positive time steps (\lambda_n), the algorithm  \left\{\begin{array}{rcl} x_1 & \in & H,\\ x_{n+1} & = & (I+\lambda_n A+\lambda_n\beta_n\partial\Psi_2)^{-1}(x_n-\lambda_n\nabla\Phi(x_n)-\lambda_n\beta_n\nabla\Psi_1(x_n)),\ n\geq 1. \end{array}\right.  performs forward steps on the smooth parts and backward steps on the other parts. Under suitable assumptions, we obtain weak ergodic convergence of the sequence (x_n) to a solution of the variational inequality. Convergence is strong when either A is strongly monotone or \Phi is strongly convex. We also obtain weak convergence of the whole sequence (x_n) when A is the subdifferential of a proper lower-semicontinuous convex function. This provides a unified setting for several classical and more recent results, in the line of historical research on continuous and discrete gradient-like systems.