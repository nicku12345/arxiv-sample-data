We consider a misspecified optimization problem that requires minimizing a function f(x;q*) over a closed and convex set X where q* is an unknown vector of parameters that may be learnt by a parallel learning process. In this context, We examine the development of coupled schemes that generate iterates {x_k,q_k} as k goes to infinity, then {x_k} converges x*, a minimizer of f(x;q*) over X and {q_k} converges to q*. In the first part of the paper, we consider the solution of problems where f is either smooth or nonsmooth under various convexity assumptions on function f. In addition, rate statements are also provided to quantify the degradation in rate resulted from learning process. In the second part of the paper, we consider the solution of misspecified monotone variational inequality problems to contend with more general equilibrium problems as well as the possibility of misspecification in the constraints. We first present a constant steplength misspecified extragradient scheme and prove its asymptotic convergence. This scheme is reliant on problem parameters (such as Lipschitz constants)and leads us to present a misspecified variant of iterative Tikhonov regularization. Numerics support the asymptotic and rate statements.