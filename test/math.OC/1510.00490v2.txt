We consider a misspecified optimization problem that requires minimizing of a convex function f(x;\theta^*) in x over a constraint set represented by h(x;\theta^*)\leq 0, where \theta^* is an unknown (or misspecified) vector of parameters. Suppose \theta^* can be learnt by a distinct process that generates a sequence of estimators \theta_k, each of which is an increasingly accurate approximation of \theta^*. We develop a first-order augmented Lagrangian scheme for computing an optimal solution x^* while simultaneously learning \theta^*.