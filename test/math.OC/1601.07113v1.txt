We first study the fast minimization properties of the trajectories of the second-order evolution equation \ddot{x}(t) + \frac{\alpha}{t} \dot{x}(t) + \beta \nabla^2 \Phi (x(t))\dot{x} (t) + \nabla \Phi (x(t)) = 0, where \Phi:\mathcal H\to\mathbb R is a smooth convex function acting on a real Hilbert space \mathcal H, and \alpha, \beta are positive parameters. This inertial system combines an isotropic viscous damping which vanishes asymptotically, and a geometrical Hessian driven damping, which makes it naturally related to Newton's and Levenberg-Marquardt methods. For \alpha\geq 3, \beta >0, along any trajectory, fast convergence of the values \Phi(x(t))- \min_{\mathcal H}\Phi =\mathcal O\left(t^{-2}\right) is obtained, together with rapid convergence of the gradients \nabla\Phi(x(t)) to zero. For \alpha>3, just assuming that \Phi has minimizers, we show that any trajectory converges weakly to a minimizer of \Phi, and  \Phi(x(t))-\min_{\mathcal H}\Phi = o(t^{-2}). Strong convergence is established in various practical situations. For the strongly convex case, convergence can be arbitrarily fast depending on the choice of \alpha. More precisely, we have \Phi(x(t))- \min_{\mathcal H}\Phi = \mathcal O(t^{-\frac{2}{3}\alpha}). We extend the results to the case of a general proper lower-semicontinuous convex function \Phi : \mathcal H \rightarrow \mathbb R \cup \{+\infty \}. This is based on the fact that the inertial dynamic with Hessian driven damping can be written as a first-order system in time and space. By explicit-implicit time discretization, this opens a gate to new - possibly more rapid - inertial algorithms, expanding the field of FISTA methods for convex structured optimization problems.