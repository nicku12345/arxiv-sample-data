This paper considers distributed average consensus using finite-bit bounded quantizer with possibly unbounded data. Under the framework of the alternating direction method of multipliers (ADMM), we develop distributed averaging algorithms where each node iteratively updates using only the local information and finitely quantized outputs from its neighbors. It is shown that all the agent variables either converge to the same quantization level or cycle around the data average after finite iterations. An error bound for the consensus value is established, which turns out to be the same as that of using the unbounded rounding quantizer provided that an algorithm parameter (i.e., ADMM step size) is small enough. We also analyze the effect of the algorithm parameter and propose an adaptive parameter selection strategy that only requires knowledge of the number of agents in order to accelerate the algorithm with certain consensus accuracy guarantee. Finally, simulations are performed to illustrate the effectiveness of the proposed algorithms.