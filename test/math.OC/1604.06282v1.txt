We study acceleration and preconditioning strategies for a class of Douglas-Rachford methods aiming at the solution of convex-concave saddle-point problems associated with Fenchel-Rockafellar duality. While the basic iteration converges weakly in Hilbert space with \mathcal{O}(1/k) ergodic convergence of restricted primal-dual gaps, acceleration can be achieved under strong-convexity assumptions. Namely, if either the primal or dual functional in the saddle-point formulation is strongly convex, then the method can be modified to yield \mathcal{O}(1/k^2) ergodic convergence. In case of both functionals being strongly convex, similar modifications lead to an asymptotic convergence of \mathcal{O}({\vartheta}^k) for some 0 < {\vartheta} < 1. All methods allow in particular for preconditioning, i.e., the inexact solution of the implicit linear step in terms of linear splitting methods with all convergence rates being maintained. The efficiency of the proposed methods is verified and compared numerically, especially showing competitiveness with respect to state-of-the-art accelerated algorithms.