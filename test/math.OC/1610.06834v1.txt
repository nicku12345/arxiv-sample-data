Projected Gradient Descent denotes a class of iterative methods for solving optimization programs. Its applicability to convex optimization programs has gained significant popularity for its intuitive implementation that involves only simple algebraic operations. In fact, if the projection onto the feasible set is easy to compute, then the method has low complexity. On the other hand, when the problem is nonconvex, e.g. because of nonlinear equality constraints, the projection becomes hard and thus impractical. In this paper, we propose a projected gradient method for Nonlinear Programs (NLPs) that only requires projections onto the linearization of the nonlinear constraints around the current iterate, similarly to Sequential Quadratic Programming (SQP). Although the projection is easier to compute, it makes the intermediate steps unfeasible for the original problem. As a result, the gradient method does not fall either into the projected gradient descent approaches, because the projection is not performed onto the original nonlinear manifold, or into the standard SQP, since second-order information is not used. For nonlinear smooth optimization problems, we analyze the similarities of the proposed method with SQP and assess its local and global convergence to a Karush-Kuhn-Tucker (KKT) point of the original problem. Further, we show that nonlinear Model Predictive Control (MPC) is a promising application of the proposed method, due to the sparsity of the resulting optimization problem. We illustrate the computational efficiency of the proposed method in a numerical example with box constraints on the control input and a quadratic terminal constraint on the state variable.