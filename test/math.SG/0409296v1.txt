In this paper we present a general framework that allows one to study discretization of certain dynamical systems. This generalizes earlier work on discretization of Lagrangian and Hamiltonian systems on tangent bundles and cotangent bundles respectively. In particular we show how to obtain a large class of discrete algorithms using this geometric approach. We give new geometric insight into the Newmark model for example and we give a direct discrete formulation of the Hamilton-Jacobi method. Moreover we extend these ideas to deriving a discrete version of the maximum principle for smooth optimal control problems. We define discrete variational principles that are the discrete counterpart of known variational principles. For dynamical systems, we introduce principles of critical action on both the tangent bundle and the cotangent bundle. These two principles are equivalent and allow one to recover most of the classical symplectic algorithms. We also identify a class of coordinate transformations that leave the variational principles presented in this paper invariant and develop a discrete Hamilton-Jacobi theory. This theory allows us to show that the energy error in the (symplectic) integration of a dynamical system is invariant under discrete canonical transformations. Finally, for optimal control problems we develop a discrete maximum principle that yields discrete necessary conditions for optimality. These conditions are in agreement with the usual conditions obtained from Pontryagin maximum principle. We illustrate our approach with an example of a sub-Riemannian optimal control.