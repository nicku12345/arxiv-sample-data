We consider a sequential Bayesian changepoint detection problem for a general stochastic model, assuming that the observed data may be dependent and non-identically distributed and the prior distribution of the change point is arbitrary, not necessarily geometric. Tartakovsky and Veeravalli (2004) developed a general asymptotic theory of changepoint detection in the non-iid case and discrete time, and Baron and Tartakovsky (2006) in continuous time assuming certain stability of the log-likelihood ratio process. This stability property was formulated in terms of the r-quick convergence of the normalized log-likelihood ratio process to a positive and finite number, which can be interpreted as the limiting Kullback-Leibler information between the "change" and "no change" hypotheses. In these papers, it was conjectured that the r-quick convergence can be relaxed in the r-complete convergence, which is typically much easier to verify in particular examples. In the present paper, we justify this conjecture by showing that the Shiryaev change detection procedure is nearly optimal, minimizing asymptotically (as the probability of false alarm vanishes) the moments of the delay to detection up to order r whenever r-complete convergence holds. We also study asymptotic properties of the Shiryaev-Roberts detection procedure in the Bayesian context.