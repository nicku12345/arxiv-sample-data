For a given parametric probability model, we consider the risk of the maximum likelihood estimator with respect to \alpha-divergence, which includes the special cases of Kullback--Leibler divergence, the Hellinger distance and \chi^2 divergence. The asymptotic expansion of the risk is given with respect to sample sizes of up to order n^{-2}. Each term in the expansion is expressed with the geometrical properties of the Riemannian manifold formed by the parametric probability model. We attempt to measure the difficulty of specifying a model through this expansion.