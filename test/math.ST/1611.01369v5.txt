We study asymptotic properties of Bayesian multiple testing procedures and provide sufficient conditions for strong consistency under general dependence structure. We also consider a novel Bayesian multiple testing procedure and associated error measures that coherently accounts for the dependence structure present in the model. We advocate posterior versions of FDR and FNR as appropriate error rates and show that their asymptotic convergence rates are directly associated with the Kullback-Leibler divergence from the true model. Our results hold even when the class of postulated models is misspecified. We illustrate our results in a variable selection problem with autoregressive response variables, and compare the new Bayesian procedure with some existing methods through extensive simulation studies in the variable selection problem. Superior performance of the new procedure compared to the others vindicate that proper exploitation of the dependence structure by multiple testing methods is indeed important. Moreover, we obtain encouraging results in a real, maize data context, where we select influential marker variables.