This paper investigates the problem of detecting relevant change points in the mean vector, say \mu_t =(\mu_{1,t},\ldots ,\mu_{d,t})^T of a high dimensional time series (Z_t)_{t\in \mathbb{Z}}.   While the recent literature on testing for change points in this context considers hypotheses for the equality of the means \mu_h^{(1)} and \mu_h^{(2)} before and after the change points in the different components, we are interested in a null hypothesis of the form  H_0: |\mu^{(1)}_{h} - \mu^{(2)}_{h} | \leq \Delta_h ~~~\mbox{ for all } ~~h=1,\ldots ,d  where \Delta_1, \ldots , \Delta_d are given thresholds for which a smaller difference of the means in the h-th component is considered to be non-relevant.   We propose a new test for this problem based on the maximum of squared and integrated CUSUM statistics and investigate its properties as the sample size n and the dimension d both converge to infinity. In particular, using Gaussian approximations for the maximum of a large number of dependent random variables, we show that on certain points of the boundary of the null hypothesis a standardised version of the maximum converges weakly to a Gumbel distribution.