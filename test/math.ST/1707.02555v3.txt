\cite{HillMotegi2017} present a new general asymptotic theory for the maximum of a random array \{\mathcal{X}_{n}(i) : 1 \leq  i \leq  \mathcal{L}\}_{n\geq 1}, where each \mathcal{X}_{n}(i) is assumed to converge in probability as n \rightarrow  \infty . The array dimension \mathcal{L} is allowed to increase with the sample size n. Existing extreme value theory arguments focus on observed data \mathcal{X}_{n}(i), and require a well defined limit law for \max_{1\leq i\leq \mathcal{L}}|\mathcal{X}_{n}(i)| by restricting dependence across i. The high dimensional central limit theory literature presumes approximability by a Gaussian law, and also restricts attention to observed data. \cite{HillMotegi2017} do not require \max_{1\leq i\leq \mathcal{L}_{n}}|\mathcal{X}_{n}(i)| to have a well defined limit nor be approximable by a Gaussian random variable, and we do not make any assumptions about dependence across i. We apply the theory to filtered data when the variable of interest \mathcal{X}_{n}(i,\theta _{0}) is not observed, but its sample counterpart \mathcal{X}_{n}(i,\hat{\theta}_{n}) is observed where \hat{\theta}_{n} estimates \theta _{0}. The main results are illustrated by looking at unit root tests for a high dimensional random variable, and a residuals white noise test.