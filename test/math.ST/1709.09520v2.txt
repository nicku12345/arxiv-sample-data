For an unknown continuous distribution on a real line, we consider the approximate estimation by the discretization. There are two methods for the discretization. First method is to divide the real line into several intervals before taking samples ("fixed interval method") . Second method is dividing the real line using the estimated percentiles after taking samples ("moving interval method"). In either way, we settle down to the estimation problem of a multinomial distribution. We use (symmetrized) f-divergence in order to measure the discrepancy of the true distribution and the estimated one. Our main result is the asymptotic expansion of the risk (i.e. expected divergence) up to the second-order term in the sample size. We prove theoretically that the moving interval method is asymptotically superior to the fixed interval method. We also observe how the presupposed intervals (fixed interval method) or percentiles (moving interval method) affect the asymptotic risk.