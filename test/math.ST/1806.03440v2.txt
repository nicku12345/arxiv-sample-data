Stochastic inversion problems are typically encountered when it is wanted to quantify the uncertainty affecting the inputs of computer models. They consist in estimating input distributions from noisy, observable outputs, and such problems are increasingly examined in Bayesian contexts where the targeted inputs are affected by a mixture of aleatory and epistemic uncertainties. While they are characterized by identifiability conditions, well-posedness constraints of "signal to noise" have to be took into account within the definition of the model, prior to inference. In addition to numeric conditioning notions and regularization techniques used in inverse problems, this article proposes and investigates a novel interpretation of well-posedness, in the context of parametric uncertainty quantification and global sensitivity analysis, based on the degradation of Fisher information. It offers an explicitation of such prior constraints considering linear or linearizable operators, this linearization being either local (based on differentiability) or variational. Simulated experiments indicate that, when injected into the modeling process, these constraints can limit the influence of measurement or process noise on the estimation of the input distribution, and let hope for future extensions in a full non-linear framework, for example through the use of linear Gaussian mixtures.