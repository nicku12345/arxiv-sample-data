Let X be a centered random vector taking values in \mathbb{R}^d and let \Sigma= \mathbb{E}(X\otimes X) be its covariance matrix. We show that if X satisfies an L_4-L_2 norm equivalence, there is a covariance estimator \hat{\Sigma} that exhibits the optimal performance one would expect had X been a gaussian vector. The procedure also improves the current state-of-the-art regarding high probability bounds in the subgaussian case (sharp results were only known in expectation or with constant probability). In both scenarios the new bound does not depend explicitly on the dimension d, but rather on the effective rank of the covariance matrix \Sigma.