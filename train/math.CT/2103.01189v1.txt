In "Backprop as functor", the authors show that the fundamental elements of deep learning -- gradient descent and backpropagation -- can be conceptualized as a strong monoidal functor \mathbf{Para}(\mathbf{Euc})\to\mathbf{Learn} from the category of parameterized Euclidean spaces to that of learners, a category developed explicitly to capture parameter update and backpropagation. It was soon realized that there is an isomorphism \mathbf{Learn}\cong\mathbf{Para}(\mathbf{SLens}), where \mathbf{SLens} is the symmetric monoidal category of simple lenses as used in functional programming.   In this note, we observe that \mathbf{SLens} is a full subcategory of \mathbf{Poly}, the category of polynomial functors in one variable, via the functor A\mapsto Ay^A. Using the fact that (\mathbf{Poly},\otimes) is monoidal closed, we show that a map A\to B in \mathbf{Para}(\mathbf{SLens}) has a natural interpretation in terms of dynamical systems (more precisely, generalized Moore machines) whose interface is the internal-hom type [Ay^A,By^B].   Finally, we review the fact that the category p\text{-}\mathbf{Coalg} of dynamical systems on any p\in\mathbf{Poly} forms a topos, and consider the logical propositions that can be stated in its internal language. We give gradient descent as an example, and we conclude by discussing some directions for future work.