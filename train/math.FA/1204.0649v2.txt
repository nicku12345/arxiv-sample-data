We study variational regularization methods in a general framework, more precisely those methods that use a discrepancy and a regularization functional. While several sets of sufficient conditions are known to obtain a regularization method, we start with an investigation of the converse question: How could necessary conditions for a variational method to provide a regularization method look like? To this end, we formalize the notion of a variational scheme and start with comparison of three different instances of variational methods. Then we focus on the data space model and investigate the role and interplay of the topological structure, the convergence notion and the discrepancy functional. Especially, we deduce necessary conditions for the discrepancy functional to fulfill usual continuity assumptions. The results are applied to discrepancy functionals given by Bregman distances and especially to the Kullback-Leibler divergence.