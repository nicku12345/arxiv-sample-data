Mixed f-divergences, a concept from information theory and statistics, measure the difference between multiple pairs of distributions. We introduce them for log concave functions and establish some of their properties. Among them are affine invariant vector entropy inequalities, like new Alexandrov-Fenchel type inequalities and an affine isoperimetric inequality for the vector form of the Kullback Leibler divergence for log concave functions. Special cases of f-divergences are mixed L_\lambda-affine surface areas for log concave functions. For those, we establish various affine isoperimetric inequalities as well as a vector Blaschke Santal\'{o} type inequality.