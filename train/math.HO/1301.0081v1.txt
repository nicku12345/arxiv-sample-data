The word "complexity" is most often used as a meta--linguistic expression referring to certain intuitive characteristics of a natural system and/or its scientific description. These characteristics may include: sheer amount of data that must be taken into account; visible "chaotic" character of these data and/or space distribution/time evolution of a system etc.   This talk is centered around the precise mathematical notion of "Kolmogorov complexity", originated in the early theoretical computer science and measuring the degree to which an available information can be compressed.   In the first part, I will argue that a characteristic feature of basic scientific theories, from Ptolemy's epicycles to the Standard Model of elementary particles, is their splitting into two very distinct parts: the part of relatively small Kolmogorov complexity ("laws", "basic equations", "periodic table", "natural selection, genotypes, mutations") and another part, of indefinitely large Kolmogorov complexity ("initial and boundary conditions", "phenotypes", "populations"). The data constituting this latter part are obtained by planned observations, focussed experiments, and afterwards collected in growing databases (formerly known as "books", "tables", "encyclopaedias" etc).   In this discussion Kolomogorov complexity plays a role of the central metaphor.   The second part and Appendix 1 are dedicated to more precise definitions and examples of complexity.