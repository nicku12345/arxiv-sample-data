This paper is focused on the performance analysis of binary linear block codes (or ensembles) whose transmission takes place over independent and memoryless parallel channels. New upper bounds on the maximum-likelihood (ML) decoding error probability are derived. The framework of the second version of the Duman and Salehi (DS2) bounds is generalized to the case of parallel channels, along with the derivation of optimized tilting measures. The connection between the generalized DS2 and the 1961 Gallager bounds, known previously for a single channel, is revisited for the case of parallel channels. The new bounds are used to obtain improved inner bounds on the attainable channel regions under ML decoding. These improved bounds are applied to ensembles of turbo-like codes, focusing on repeat-accumulate codes and their recent variations.