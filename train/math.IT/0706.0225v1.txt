In this paper, we study the end-to-end distortion/delay tradeoff for a analogue source transmitted over a fading channel. The analogue source is quantized and stored in a buffer until it is transmitted. There are two extreme cases as far as buffer delay is concerned: no delay and infinite delay. We observe that there is a significant power gain by introducing a buffer delay. Our goal is to investigate the situation between these two extremes. Using recently proposed \emph{effective capacity} concept, we derive a closed-form formula for this tradeoff. For SISO case, an asymptotically tight upper bound for our distortion-delay curve is derived, which approaches to the infinite delay lower bound as \mathcal{D}_\infty \exp(\frac{C}{\tau_n}), with \tau_n is the normalized delay, C is a constant. For more general MIMO channel, we computed the distortion SNR exponent -- the exponential decay rate of the expected distortion in the high SNR regime. Numerical results demonstrate that introduction of a small amount delay can save significant transmission power.