There are three classical divergence measures exist in the literature on information theory and statistics. These are namely, Jeffryes-Kullback-Leiber J-divergence. Sibson-Burbea-Rao Jensen-Shannon divegernce and Taneja arithemtic-geometric mean divergence. These three measures bear an interesting relationship among each other and are based on logarithmic expressions. The divergence measures like Hellinger discrimination, symmetric chi-square divergence, and triangular discrimination are also known in the literature and are not based on logarithmic expressions. Past years Dragomir et al., Kumar and Johnson and Jain and Srivastava studied different kind of divergence measures. In this paper, we have presented some more new divergence measures and obtained inequalities relating these new measures and also made connections with previous ones. The idea of exponential divergence is also introduced.