The optimal diversity-multiplexing-delay tradeoff for the multi-input multi-output (MIMO) automatic repeat request (ARQ) channel can be achieved using incremental redundancy lattice space-time codes coupled with a list decoder for joint error detection and correction. Such a decoder is based on the minimum mean-square error lattice decoding principle which is implemented using sphere decoding algorithms. However, sphere decoders suffer from high computational complexity for low-to-moderate signal-to-noise ratios, especially for large signal dimensions. In this paper, we would like to construct a more efficient decoder that is capable of achieving the optimal tradeoff with much lower complexity. In particular, we will study the throughput-performance-complexity tradeoffs in sequential decoding algorithms and the effect of preprocessing and termination strategies. We show, analytically and via simulation, that using the \textit{lattice sequential decoder} that implements a time-out algorithm for joint error detection and correction, the optimal tradeoff of the MIMO ARQ channel can be achieved with significant reduction in decoding complexity.