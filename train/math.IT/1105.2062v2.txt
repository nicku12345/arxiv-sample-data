The distortion-rate performance of certain randomly-designed scalar quantizers is determined. The central results are the mean-squared error distortion and output entropy for quantizing a uniform random variable with thresholds drawn independently from a uniform distribution. The distortion is at most 6 times that of an optimal (deterministically-designed) quantizer, and for a large number of levels the output entropy is reduced by approximately (1-gamma)/(ln 2) bits, where gamma is the Euler-Mascheroni constant. This shows that the high-rate asymptotic distortion of these quantizers in an entropy-constrained context is worse than the optimal quantizer by at most a factor of 6 exp(-2(1-gamma)).