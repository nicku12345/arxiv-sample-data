Compressed sensing is a signal processing technique in which data is acquired directly in a compressed form. There are two modeling approaches that can be considered: the worst-case (Hamming) approach and a statistical mechanism, in which the signals are modeled as random processes rather than as individual sequences. In this paper, the second approach is studied. In particular, we consider a model of the form \boldsymbol{Y} = \boldsymbol{H}\boldsymbol{X}+\boldsymbol{W}, where each comportment of \boldsymbol{X} is given by X_i = S_iU_i, where \left\{U_i\right\} are i.i.d. Gaussian random variables, and \left\{S_i\right\} are binary random variables independent of \left\{U_i\right\}, and not necessarily independent and identically distributed (i.i.d.), \boldsymbol{H}\in\mathbb{R}^{k\times n} is a random matrix with i.i.d. entries, and \boldsymbol{W} is white Gaussian noise. Using a direct relationship between optimum estimation and certain partition functions, and by invoking methods from statistical mechanics and from random matrix theory (RMT), we derive an asymptotic formula for the minimum mean-square error (MMSE) of estimating the input vector \boldsymbol{X} given \boldsymbol{Y} and \boldsymbol{H}, as k,n\to\infty, keeping the measurement rate, R = k/n, fixed. In contrast to previous derivations, which are based on the replica method, the analysis carried out in this paper is rigorous.