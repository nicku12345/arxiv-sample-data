We generalize the ham sandwich theorem to d+1 measures in \mathbb{R}^d as follows. Let \mu_1,\mu_2, \dots, \mu_{d+1} be absolutely continuous finite Borel measures on \mathbb{R}^d. Let \omega_i=\mu_i(\mathbb{R}^d) for i\in [d+1], \omega=\min\{\omega_i; i\in [d+1]\} and assume that \sum_{j=1}^{d+1} \omega_j=1. Assume that \omega_i \le 1/d for every i\in[d+1]. Then there exists a hyperplane h such that each open halfspace H defined by h satisfies \mu_i(H) \le (\sum_{j=1}^{d+1} \mu_j(H))/d for every i \in [d+1] and \sum_{j=1}^{d+1} \mu_j(H) \ge \min(1/2, 1-d\omega) \ge 1/(d+1). As a consequence we obtain that every (d+1)-colored set of nd points in \mathbb{R}^d such that no color is used for more than n points can be partitioned into n disjoint rainbow (d-1)-dimensional simplices.