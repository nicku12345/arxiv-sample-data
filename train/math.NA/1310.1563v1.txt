Optimization of convex functions subject to eigenvalue constraints is intriguing because of peculiar analytical properties of eigenvalues, and is of practical interest because of wide range of applications in fields such as structural design and control theory. Here we focus on the optimization of a linear objective subject to a constraint on the smallest eigenvalue of an analytical and Hermitian matrix-valued function. We offer a quadratic support function based numerical solution. The quadratic support functions are derived utilizing the variational properties of an eigenvalue over a set of Hermitian matrices. Then we establish the local convergence of the algorithm under mild assumptions, and deduce a precise rate of convergence result by viewing the algorithm as a fixed point iteration. We illustrate its applicability in practice on the pseudospectral functions.