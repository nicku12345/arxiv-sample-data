We consider \mathbb{L}_2-approximation of elements of a Hermite space of analytic functions over \mathbb{R}^s. The Hermite space is a weighted reproducing kernel Hilbert space of real valued functions for which the Hermite coefficients decay exponentially fast. The weights are defined in terms of two sequences \boldsymbol{a} = \{a_j\} and \boldsymbol{b} = \{b_j\} of positive real numbers. We study the nth minimal worst-case error e(n,{\rm APP}_s;\Lambda^{{\rm std}}) of all algorithms that use n information evaluations from the class \Lambda^{{\rm std}} which only allows function evaluations to be used.   We study (uniform) exponential convergence of the nth minimal worst-case error, which means that e(n,{\rm APP}_s; \Lambda^{{\rm std}}) converges to zero exponentially fast with increasing n. Furthermore, we consider how the error depends on the dimension s. To this end, we study the minimal number of information evaluations needed to compute an \varepsilon-approximation by considering several notions of tractability which are defined with respect to s and \log \varepsilon^{-1}. We derive necessary and sufficient conditions on the sequences \boldsymbol{a} and \boldsymbol{b} for obtaining exponential error convergence, and also for obtaining the various notions of tractability. It turns out that the conditions on the weight sequences are almost the same as for the information class \Lambda^{{\rm all}} which uses all linear functionals. The results are also constructive as the considered algorithms are based on tensor products of Gauss-Hermite rules for multivariate integration. The obtained results are compared with the analogous results for integration in the same Hermite space. This allows us to give a new sufficient condition for EC-weak tractability for integration.