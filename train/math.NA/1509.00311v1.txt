We consider the problem of fitting a low rank tensor A\in\mathbb{R}^{{\mathcal I}}, {\mathcal I} = \{1,\ldots,n\}^{d}, to a given set of data points \{M_i\in\mathbb{R}\mid i\in P\}, P\subset{\mathcal I}.   The low rank format under consideration is the hierarchical or TT or MPS format. It is characterized by rank bounds r on certain matricizations of the tensor. The number of degrees of freedom is in {\cal O}(r^2dn).   For a fixed rank and mode size n we observe that it is possible to reconstruct random (but rank structured) tensors as well as certain discretized multivariate (but rank structured) functions from a number of samples that is in {\cal O}(\log N) for a tensor having N=n^d entries.   We compare an alternating least squares fit (ALS) to an overrelaxation scheme inspired by the LMaFit method for matrix completion.   Both approaches aim at finding a tensor A that fulfils the first order optimality conditions by a nonlinear Gauss-Seidel type solver that consists of an alternating fit cycling through the directions \mu=1,\ldots,d.   The least squares fit is of complexity {\cal O}(r^4d\#P) per step, whereas each step of ADF is in {\cal O}(r^2d\#P), albeit with a slightly higher number of necessary steps.   In the numerical experiments we observe robustness of the completion algorithm with respect to noise and good reconstruction capability.   Our tests provide evidence that the algorithm is suitable in higher dimension (>10) as well as for moderate ranks.   Keywords: MPS, Tensor Completion, Tensor Train, TT, Hierarchical Tucker, HT, ALS.