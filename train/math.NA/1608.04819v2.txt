Popular methods for finding regularized solutions to inverse problems include sparsity promoting \ell_1 regularization techniques, one in particular which is the well known total variation (TV) regularization. More recently, several higher order (HO) methods similar to TV have been proposed, which we generally refer to as HOTV methods. In this letter, we investigate problem of the often debated selection of \lambda, the parameter used to carefully balance the interplay between data fitting and regularization terms. We theoretically argue for a scaling of the parameter that works for all orders for HOTV methods, based off of a single selection of the parameter for any one of the orders. We also provide numerical results which justify our theoretical findings.