We consider the convex optimization problem \min \{f(x) : g_j(x)\leq 0, j=1,...,m\} where f is convex, the feasible set K is convex and Slater's condition holds, but the functions g_j are not necessarily convex. We show that for any representation of K that satisfies a mild nondegeneracy assumption, every minimizer is a Karush-Kuhn-Tucker (KKT) point and conversely every KKT point is a minimizer. That is, the KKT optimality conditions are necessary and sufficient as in convex programming where one assumes that the g_j are convex. So in convex optimization, and as far as one is concerned with KKT points, what really matters is the geometry of K and not so much its representation.