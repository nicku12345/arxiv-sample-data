Many convex optimization methods are conceived of and analyzed in a largely separate fashion. In contrast to this traditional separation, this manuscript points out and demonstrates the utility of an important but largely unremarked common thread running through many prominent optimization methods. In particular, we show that methods such as successive orthogonal projection, gradient descent, projected gradient descent, the proximal-point method, forward-backward splitting, the alternating direction method of multipliers, and under- or over-relaxed variants of the preceding all involve updates that are of a common type --- namely, the updates satisfy a property known as pseudocontractivity. Moreover, since the property of pseudocontractivity is preserved under both composition and convex combination, updates constructed via these operations from pseudocontractive updates are themselves pseudocontractive. Having demonstrated that pseudocontractive updates are to be found in many optimization methods, we then provide a unified basic analysis of methods with pseudocontractive updates. Specifically, we prove a novel bound satisfied by the norm of the difference in iterates of pseudocontractive updates and we then use this bound to establish that the error criterion \left\Vert x^{N}-Tx^{N}\right\Vert ^{2} is o(1/N) for any method involving pseudocontractive updates (where N is the number of iterations and T is the iteration operator).