A general representation of the quaternion gradients presented in the literature is proposed, and an universal update equation for QLMS-like algorithms is obtained. The general update law is used to study the convergence of widely linear (WL) algorithms. It is proved that techniques obtained with a gradient similar to the i-gradient are the fastest-converging in two situations: 1) When the correlation matrix contains elements only in 2 axis (1 and i, for instance), and 2) When the algorithms use a real data vector, obtained staking up the real and imaginary parts of the original quaternion input vector. The general update law is also used to study the convergence of WL-QLMS-based algorithms, and an accurate second-order model is developed for quaternion algorithms using real-data input. Based on the proposed analysis, we obtain the fastest-converging WL-QLMS algorithm with real-regressor vector, which is also less costly than the reduced-complexity WL-QLMS (RC-WL-QLMS) algorithm proposed in our previous work. It is shown that the new method corresponds to the four-channel LMS algorithm written in the quaternion domain, and that they have the same computational complexity. Simulations illustrate the performance of the new technique and the accuracy of the analysis.