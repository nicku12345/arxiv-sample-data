We present a preconditioning of a generalized forward-backward splitting algorithm for finding a zero of a sum of maximally monotone operators \sum_{i=1}^{n} A_i + B with B cocoercive, involving only the computation of B and of the resolvent of each A_i separately. This allows in particular to minimize functionals of the form \sum_{i=1}^n g_i + f with f smooth, using only the computation of the gradient of f and of the proximity operator of each g_i separately. By adapting the underlying metric, such preconditioning can serve two practical purposes: first, it might accelerate the convergence, or second, it might simplify the computation of the resolvent of A_i for some i. In addition, in many cases of interest, our preconditioning strategy allows the economy of storage and computation concerning some auxiliary variables. In particular, we show how this approach can handle large-scale, nonsmooth, convex optimization problems structured on graphs, which arises in many image processing or learning applications, and that it compares favorably to alternatives in the literature.