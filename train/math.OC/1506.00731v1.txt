In complex engineered systems, completing an objective is sometimes not enough. The system must be able to reach a set performance characteristic, such as an unmanned aerial vehicle flying from point A to point B, \textit{under 10 seconds}. This introduces the notion of optimality, what is the most efficient, the fastest, the cheapest way to complete a task. This report explores the two pillars of optimal control, Bellman's Dynamic Programming and Pontryagin's Maximum Principle, and compares implementations of both theories onto simulated systems. Dynamic Programming is realized through a Differential Dynamic Programming Algorithm, where utilizes a forward-backward pass to iteratively optimize a control sequence and trajectory. The Maximum Principle is realized via Gauss Pseudospectral Optimal Control, where the optimal control problem is first approximated through polynomial basis functions, then solved, with optimality being achieved through the costate equations of the Maximum Principle. The results of the report show that, for short time Horizons, DDP can optimize quickly and can generate a trajectory that utilizes less control effort for the same problem formulation. On the other hand Pseudospectral methods can optimize faster for longer time horizons, but require a key understanding of the problem structure. Future work involves completing an implementation of DDP in a C++ code, and testing the speed of convergence for both methods, as well as extended the Pseudospectral Optimal Control framework in to the world of stochastic optimal control.