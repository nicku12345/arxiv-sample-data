We consider the minimization problems of the form P(\varphi, g, h): \min\{f(x) = \varphi(x) + g(x) - h(x): x \in \Bbb R^n\}, where \varphi is a differentiable function and g, h are convex functions, and introduce iterative methods to finding a critical point of f when f is differentiable. We show that the point computed by proximal point algorithm at each iteration can be used to determine a descent direction for the objective function at this point. This algorithm can be considered as a combination of proximal point algorithm together with a linesearch step that uses this descent direction. We also study convergence results of these algorithms and the inertial proximal methods proposed by P.E. Maing\acute{e} {\it et.al.} \cite{MM} under the main assumption that the objective function satisfies the Kurdika-{\L}ojasiewicz property.