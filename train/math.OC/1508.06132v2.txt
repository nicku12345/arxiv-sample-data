We provide a numerical scheme to approximate as closely as desired the Gaussian or exponential measure \mu(\om) of (not necessarily compact) basic semi-algebraic sets\om\subset\R^n. We obtain two monotone (non increasing and non decreasing) sequences of upper and lower bounds (\overline{\omega}\_d), (\underline{\omega}\_d), d\in\N, each converging to \mu(\om) as d\to\infty. For each d, computing \overline{\omega}\_d or \underline{\omega}\_dreduces to solving a semidefinite program whose size increases with d. Some preliminary (small dimension) computational experiments are encouraging and illustrate thepotential of the method. The method also works for any measure whose moments are known and which satisfies Carleman's condition.