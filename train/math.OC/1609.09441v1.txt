We consider minimizing the composite function that consists of a strongly convex function and a convex function. The fast dual proximal gradient (FDPG) method decreases the dual function with a rate O(1/k^2), leading to a rate O(1/k) for decreasing the primal function. We propose a generalized FDPG method that guarantees an O(1/k^{1.5}) rate for the dual proximal gradient norm decrease. By relating this to the primal function decrease, the proposed approach decreases the primal function with the improved O(1/k^{1.5}) rate.