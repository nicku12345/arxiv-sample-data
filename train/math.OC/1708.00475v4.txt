An algorithm for solving smooth nonconvex optimization problems is proposed that, in the worst-case, takes \mathcal{O}(\epsilon^{-3/2}) iterations to drive the norm of the gradient of the objective function below a prescribed positive real number \epsilon and can take \mathcal{O}(\epsilon^{-3}) iterations to drive the leftmost eigenvalue of the Hessian of the objective above -\epsilon. The proposed algorithm is a general framework that covers a wide range of techniques including quadratically and cubically regularized Newton methods, such as the Adaptive Regularisation using Cubics (ARC) method and the recently proposed Trust-Region Algorithm with Contractions and Expansions (TRACE). The generality of our method is achieved through the introduction of generic conditions that each trial step is required to satisfy, which in particular allow for inexact regularized Newton steps to be used. These conditions center around a new subproblem that can be approximately solved to obtain trial steps that satisfy the conditions. A new instance of the framework, distinct from ARC and TRACE, is described that may be viewed as a hybrid between quadratically and cubically regularized Newton methods. Numerical results demonstrate that our hybrid algorithm outperforms a cublicly regularized Newton method.