We study the convergence of the Augmented Decomposition Algorithm (ADA) proposed in [32] for solving multi-block separable convex minimization problems subject to linear constraints. We show that the global convergence rate of the exact ADA is o(1/k) under the assumption that there exists a saddle point. We consider the inexact Augmented Decomposition Algorithm (iADA) and establish global and local convergence results under some mild assumptions, by providing a stability result for the maximal monotone operator T associated with the perturbation from both primal and dual perspectives. This result implies the local linear convergence of the inexact ADA for many applications such as the lasso, total variation reconstruction, exchange problem and many other problems from statistics, machine learning and engineering with l1 regularization.