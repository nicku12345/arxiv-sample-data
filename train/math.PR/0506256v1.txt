There are many information and divergence measures exist in the literature on information theory and statistics. The most famous among them are Kullback-Leibler (1951) relative information and Jeffreys (1951) J-divergence. Sibson (1969) Jensen-Shannon divergence has also found its applications in the literature. The author (1995) studied a new divergence measures based on arithmetic and geometric means. The measures like harmonic mean divergence and triangular discrimination are also known in the literature. Recently, Dragomir et al. (2001) also studies a new measure similar to J-divergence, we call here the relative J-divergence. Another measures arising due to Jensen-Shannon divergence is also studied by Lin (1991). Here we call it relative Jensen-Shannon divergence. Relative arithmetic-geometric divergence (ref. Taneja, 2004) is also studied here. All these measures can be written as particular cases of Csiszar's f-divergence. By putting some conditions on the probability distribution, the aim here is to obtain bounds among the measures.