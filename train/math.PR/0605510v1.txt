In this paper we revisit the Bialynicki-Birula & Mycielski uncertainty principle and its cases of equality. This Shannon entropic version of the well-known Heisenberg uncertainty principle can be used when dealing with variables that admit no variance. In this paper, we extend this uncertainty principle to Renyi entropies. We recall that in both Shannon and Renyi cases, and for a given dimension n, the only case of equality occurs for Gaussian random vectors. We show that as n grows, however, the bound is also asymptotically attained in the cases of n-dimensional Student-t and Student-r distributions. A complete analytical study is performed in a special case of a Student-t distribution. We also show numerically that this effect exists for the particular case of a n-dimensional Cauchy variable, whatever the Renyi entropy considered, extending the results of Abe and illustrating the analytical asymptotic study of the student-t case. In the Student-r case, we show numerically that the same behavior occurs for uniformly distributed vectors. These particular cases and other ones investigated in this paper are interesting since they show that this asymptotic behavior cannot be considered as a "Gaussianization" of the vector when the dimension increases.