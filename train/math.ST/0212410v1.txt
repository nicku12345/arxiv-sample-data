State space models have long played an important role in signal processing. The Gaussian case can be treated algorithmically using the famous Kalman filter. Similarly since the 1970s there has been extensive application of Hidden Markov models in speech recognition with prediction being the most important goal. The basic theoretical work here, in the case X and Y finite (small) providing both algorithms and asymptotic analysis for inference is that of Baum and colleagues. During the last 30-40 years these general models have proved of great value in applications ranging from genomics to finance.   Unless the X,Y are jointly Gaussian or X is finite and small the problem of calculating the distributions discussed and the likelihood exactly are numerically intractable and if Y is not finite asymptotic analysis becomes much more difficult. Some new developments have been the construction of so-called ``particle filters'' (Monte Carlo type) methods for approximate calculation of these distributions (see Doucet et al. [4]) for instance and general asymptotic methods for analysis of statistical methods in HMM [2] and other authors.   We will discuss these methods and results in the light of exponential mixing properties of the conditional (posterior) distribution of (X_1,X_2,...) given (Y_1,Y_2,...) already noted by Baum and Petrie and recent work of the authors Bickel, Ritov and Ryden, Del Moral and Jacod, Douc and Matias.