There are three classical divergence measures known in the literature on information theory and statistics. These are namely, Jeffryes-Kullback-Leiber \cite{jef} \cite{kul} \textit{J-divergence}. Sibson-Burbea-Rao \cite{sib} \cite{bur1, bur2} \textit{Jensen-Shannon divegernce}and Taneja \cite{tan3} \textit{Arithemtic-Geometric divergence}. These three measures bears an interesting relationship among each other. The divergence measures like \textit{Hellinger discrimination}, \textit{symmetric}\chi ^2 - \textit{divergence}, and \textit{triangular discrimination} are also known in the literature. All these measures can be written as particular cases of Csisz\'{a}r's \textit{f-divergence}. Recently, author proved an inequality relating all the six measures. In this paper our aim is to give one parametric generalizations of the above measures and established relationships among them. A new measure similar to \textit{Hellinger's} and \textit{triangular discriminations} is also derived.