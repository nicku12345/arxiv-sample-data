The authors consider the problem of estimating the density g of independent and identically distributed variables X\_i, from a sample Z\_1, ..., Z\_n where Z\_i=X\_i+\sigma\epsilon\_i, i=1, ..., n, \epsilon is a noise independent of X, with \sigma\epsilon having known distribution. They present a model selection procedure allowing to construct an adaptive estimator of g and to find non-asymptotic bounds for its \mathbb{L}\_2(\mathbb{R})-risk. The estimator achieves the minimax rate of convergence, in most cases where lowers bounds are available. A simulation study gives an illustration of the good practical performances of the method.