In the sequential change-point detection literature, most research specifies a required frequency of false alarms at a given pre-change distribution f_{\theta} and tries to minimize the detection delay for every possible post-change distribution g_{\lambda}. In this paper, motivated by a number of practical examples, we first consider the reverse question by specifying a required detection delay at a given post-change distribution and trying to minimize the frequency of false alarms for every possible pre-change distribution f_{\theta}. We present asymptotically optimal procedures for one-parameter exponential families. Next, we develop a general theory for change-point problems when both the pre-change distribution f_{\theta} and the post-change distribution g_{\lambda} involve unknown parameters. We also apply our approach to the special case of detecting shifts in the mean of independent normal observations.