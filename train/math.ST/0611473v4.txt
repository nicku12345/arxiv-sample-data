In the context of density level set estimation, we study the convergence of general plug-in methods under two main assumptions on the density for a given level \lambda. More precisely, it is assumed that the density (i) is smooth in a neighborhood of \lambda and (ii) has \gamma-exponent at level \lambda. Condition (i) ensures that the density can be estimated at a standard nonparametric rate and condition (ii) is similar to Tsybakov's margin assumption which is stated for the classification framework. Under these assumptions, we derive optimal rates of convergence for plug-in estimators. Explicit convergence rates are given for plug-in estimators based on kernel density estimators when the underlying measure is the Lebesgue measure. Lower bounds proving optimality of the rates in a minimax sense when the density is H\"older smooth are also provided.