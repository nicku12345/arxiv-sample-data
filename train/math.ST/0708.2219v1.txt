We aim at estimating a function \lambda:[0,1]\to \mathbb {R}, subject to the constraint that it is decreasing (or increasing). We provide a unified approach for studying the \mathbb {L}_p-loss of an estimator defined as the slope of a concave (or convex) approximation of an estimator of a primitive of \lambda, based on n observations. Our main task is to prove that the \mathbb {L}_p-loss is asymptotically Gaussian with explicit (though unknown) asymptotic mean and variance. We also prove that the local \mathbb {L}_p-risk at a fixed point and the global \mathbb {L}_p-risk are of order n^{-p/3}. Applying the results to the density and regression models, we recover and generalize known results about Grenander and Brunk estimators. Also, we obtain new results for the Huang--Wellner estimator of a monotone failure rate in the random censorship model, and for an estimator of the monotone intensity function of an inhomogeneous Poisson process.