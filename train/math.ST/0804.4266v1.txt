We study the problem of estimating the coefficients in linear ordinary differential equations (ODE's) with a diverging number of variables when the solutions are observed with noise. The solution trajectories are first smoothed with local polynomial regression and the coefficients are estimated with nonconcave penalty proposed by \cite{fan01}. Under some regularity and sparsity conditions, we show the procedure can correctly identifies nonzero coefficients with probability converging to one and the estimators for nonzero coefficients have the same asymptotic normal distribution as they would have when the zero coefficients are known and the same two-step procedure is used. Our asymptotic results are valid under the misspecified case where linear ODE's are only used as an approximation to nonlinear ODE's, and the estimates will converge to the coefficients of the best approximating linear system. From our results, when the solution trajectories of the ODE's are sufficiently smooth, the parametric \sqrt{n} rate is achieved even though nonparametric regression estimator is used in the first step of the procedure. The performance of the two-step procedure is illustrated by a simulation study as well as an application to yeast cell-cycle data.