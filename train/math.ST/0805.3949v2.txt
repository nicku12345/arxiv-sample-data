Given a collection of computational models that all estimate values of the same natural process, we compare the performance of the average of the collection to the individual member whose estimates are nearest a given set of observations. Performance is the ability of a model, or average, to reproduce a sequence of observations of the process. We identify a condition that determines if a single model performs better than the average. That result also yields a necessary condition for when the average performs better than any individual model. We also give sharp bounds for the performance of the average on a given interval. Since the observation interval is fixed, performance is evaluated in a vector space, and we can add intuition to our results by explaining them geometrically. We conclude with some comments on directions statistical tests of performance might take.