In this paper, we investigate the theoretical and empirical properties of L_2 boosting with kernel regression estimates as weak learners. We show that each step of L_2 boosting reduces the bias of the estimate by two orders of magnitude, while it does not deteriorate the order of the variance. We illustrate the theoretical findings by some simulated examples. Also, we demonstrate that L_2 boosting is superior to the use of higher-order kernels, which is a well-known method of reducing the bias of the kernel estimate.