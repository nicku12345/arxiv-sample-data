We present an argument based on the multidimensional and the uniform central limit theorems, proving that, under some geometrical assumptions between the target function T and the learning class F, the excess risk of the empirical risk minimization algorithm is lower bounded by \[\frac{\mathbb{E}\sup_{q\in Q}G_q}{\sqrt{n}}\delta,\] where (G_q)_{q\in Q} is a canonical Gaussian process associated with Q (a well chosen subset of F) and \delta is a parameter governing the oscillations of the empirical excess risk function over a small ball in F.