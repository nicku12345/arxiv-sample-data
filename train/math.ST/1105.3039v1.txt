A general lower bound is developed for the minimax risk when estimating an arbitrary functional. The bound is based on testing two composite hypotheses and is shown to be effective in estimating the nonsmooth functional {\frac{1}{n}}\sum|\theta_i| from an observation Y\sim N(\theta,I_n). This problem exhibits some features that are significantly different from those that occur in estimating conventional smooth functionals. This is a setting where standard techniques fail to yield sharp results. A sharp minimax lower bound is established by applying the general lower bound technique based on testing two composite hypotheses. A key step is the construction of two special priors and bounding the chi-square distance between two normal mixtures. An estimator is constructed using approximation theory and Hermite polynomials and is shown to be asymptotically sharp minimax when the means are bounded by a given value M. It is shown that the minimax risk equals \beta_*^2M^2({\frac{\log\log n}{\log n}})^2 asymptotically, where \beta_* is the Bernstein constant. The general techniques and results developed in the present paper can also be used to solve other related problems.