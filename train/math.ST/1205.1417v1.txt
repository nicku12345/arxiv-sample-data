The effect of errors in variables in empirical minimization is investigated. Given a loss l and a set of decision rules \mathcal{G}, we prove a general upper bound for an empirical minimization based on a deconvolution kernel and a noisy sample Z_i=X_i+\epsilon_i,i=1,...,n. We apply this general upper bound to give the rate of convergence for the expected excess risk in noisy clustering. A recent bound from \citet{levrard} proves that this rate is \mathcal{O}(1/n) in the direct case, under Pollard's regularity assumptions. Here the effect of noisy measurements gives a rate of the form \mathcal{O}(1/n^{\frac{\gamma}{\gamma+2\beta}}), where \gamma is the H\"older regularity of the density of X whereas \beta is the degree of illposedness.