We address the choice of the tuning parameter \lambda in \ell_1-penalized M-estimation. Our main concern is models which are highly nonlinear, such as the Gaussian mixture model. The number of parameters p is moreover large, possibly larger than the number of observations n. The generic chaining technique of Talagrand[2005] is tailored for this problem. It leads to the choice \lambda \asymp \sqrt {\log p / n}, as in the standard Lasso procedure (which concerns the linear model and least squares loss).