Consider a nonlinear regression model : y_{i}=g(x_{i},{\theta})+e_{i}, i=1,...,n, where the x_{i} are random predictors x_{i} and {\theta} is the unknown parameter vector ranging in a set {\Theta}\subsetR^{p}. All known results on the consistency of the least squares estimator and in general of M estimators assume that either {\Theta} is compact or g is bounded, which excludes frequently employed models such as the Michaelis-Menten, logistic growth and exponential decay models. In this article we deal with the so-called separable models, where p=p_{1}+p_{2}, {\theta}=({\alpha},{\beta}) with {\alpha}\inA\subsetR^{p_{1}}, {\beta}\inB\subsetR^{p_{2},}and g has the form g(x,{\theta})={\beta}^{T}h(x,{\alpha}) where h is a function with values in R^{p_{2}}. We prove the strong consistency of M estimators under very general assumptions, assuming that h is a bounded function of {\alpha}, which includes the three models mentioned above. Key words and phrases: Nonlinear regression, separable models, consistency, robust estimation.