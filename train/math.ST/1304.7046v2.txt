The incorporation of priors in the Optimal Uncertainty Quantification (OUQ) framework \cite{OSSMO:2011} reveals brittleness in Bayesian inference; a model may share an arbitrarily large number of finite-dimensional marginals with, or be arbitrarily close (in Prokhorov or total variation metrics) to, the data-generating distribution and still make the largest possible prediction error after conditioning on an arbitrarily large number of samples. The initial purpose of this paper is to unwrap this brittleness mechanism by providing (i) a quantitative version of the Brittleness Theorem of \cite{BayesOUQ} and (ii) a detailed and comprehensive analysis of its application to the revealing example of estimating the mean of a random variable on the unit interval [0,1] using priors that exactly capture the distribution of an arbitrarily large number of Hausdorff moments.   However, in doing so, we discovered that the free parameter associated with Markov and Kre\u{\i}n's canonical representations of truncated Hausdorff moments generates reproducing kernel identities corresponding to reproducing kernel Hilbert spaces of polynomials.   Furthermore, these reproducing identities lead to biorthogonal systems of Selberg integral formulas.   This process of discovery appears to be generic: whereas Karlin and Shapley used Selberg's integral formula to first compute the volume of the Hausdorff moment space (the polytope defined by the first n moments of a probability measure on the interval [0,1]), we observe that the computation of that volume along with higher order moments of the uniform measure on the moment space, using different finite-dimensional representations of subsets of the infinite-dimensional set of probability measures on [0,1] representing the first n moments, leads to families of equalities corresponding to classical and new Selberg identities.