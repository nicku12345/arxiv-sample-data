In this work, we are concerned with the estimation of the predictive density of a Gaussian random vector where both the mean and the variance are unknown. In such a context, we prove the inadmissibility of the best equivariant predictive density under the Kullback-Leibler risk in a nonasymptotic framework. Our result stands whatever the dimension d of the vector is, even when d<=2, which can be somewhat surprising compared to the known variance setting. We also propose a class of priors leading to a Bayesian predictive density that dominates the best equivariant one. Throughout the article, we give several elements that we believe are useful for establishing the parallel between the prediction and the estimation problems, as it was done in the known variance framework.