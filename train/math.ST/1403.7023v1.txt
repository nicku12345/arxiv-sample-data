We examine the rate of convergence of the Lasso estimator of lower dimensional components of the high-dimensional parameter. Under bounds on the \ell_1-norm on the worst possible sub-direction these rates are of order \sqrt {|J| \log p / n } where p is the total number of parameters, J \subset \{ 1, \ldots, p \} represents a subset of the parameters and n is the number of observations. We also derive rates in sup-norm in terms of the rate of convergence in \ell_1-norm. The irrepresentable condition on a set J requires that the \ell_1-norm of the worst possible sub-direction is sufficiently smaller than one. In that case sharp oracle results can be obtained. Moreover, if the coefficients in J are small enough the Lasso will put these coefficients to zero. This extends known results which say that the irrepresentable condition on the inactive set (the set where coefficients are exactly zero) implies no false positives. We further show that by de-sparsifying one obtains fast rates in supremum norm without conditions on the worst possible sub-direction. The main assumption here is that approximate sparsity is of order o (\sqrt n / \log p ). The results are extended to M-estimation with \ell_1-penalty for generalized linear models and exponential families for example. For the graphical Lasso this leads to an extension of known results to the case where the precision matrix is only approximately sparse. The bounds we provide are non-asymptotic but we also present asymptotic formulations for ease of interpretation.