We consider an additive regression model consisting of two components f^0 and g^0, where the first component f^0 is in some sense "smoother" than the second g^0. Smoothness is here described in terms of a semi-norm on the class of regression functions. We use a penalized least squares estimator (\hat f, \hat g) of (f^0, g^0) and show that the rate of convergence for \hat f  is faster than the rate of convergence for \hat g. In fact, both rates are generally as fast as in the case where one of the two components is known. The theory is illustrated by a simulation study. Our proofs rely on recent results from empirical process theory.