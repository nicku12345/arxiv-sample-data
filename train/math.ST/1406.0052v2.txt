We consider the problem of variable selection in high-dimensional sparse additive models. We focus on the case that the components belong to nonparametric classes of functions. The proposed method is motivated by geometric considerations in Hilbert spaces and consists of comparing the norms of the projections of the data onto various additive subspaces. Under minimal geometric assumptions, we prove concentration inequalities which lead to new conditions under which consistent variable selection is possible. As an application, we establish conditions under which a single component can be estimated with the rate of convergence corresponding to the situation in which the other components are known.