Suppose we have data generated according to a multivariate normal distribution with a fixed unknown mean vector that is sparse in the sense of being nearly black. Optimality of Bayes estimates and posterior concentration properties in terms of the minimax risk in the l_2 norm corresponding to a very general class of continuous shrinkage priors are studied in this work. The class of priors considered is rich enough to include a great variety of heavy tailed prior distributions, such as, the three parameter beta normal mixtures (including the horseshoe), the generalized double Pareto, the inverse gamma and the normal-exponential-gamma priors. Assuming that the number of non-zero components of the mean vector is known, we show that the Bayes estimators corresponding to this general class of priors attain the minimax risk in the l_2 norm (possibly up to a multiplicative constant) and the corresponding posterior distributions contract around the true mean vector at the minimax optimal rate for appropriate choice of the global shrinkage parameter. Moreover, we provide conditions for which these posterior distributions contract around the corresponding Bayes estimates at least as fast as the minimax risk in the l_2 norm. We also provide a lower bound to the total posterior variance for an important subclass of this general class of shrinkage priors that includes the generalized double Pareto priors with shape parameter \alpha=0.5 and the three parameter beta normal mixtures with parameters a=0.5 and b>0 (including the horseshoe) in particular. The present work is inspired by the recent work of van der Pas et al. (2014) on the posterior contraction properties of the horseshoe prior under the present set-up. We extend their results for this general class of priors and come up with novel unifying proofs which work for a very broad class of one-group continuous shrinkage priors.