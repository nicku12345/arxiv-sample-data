For a convex class of functions F, a regularization functions \Psi(\cdot) and given the random data (X_i, Y_i)_{i=1}^N, we study estimation properties of regularization procedures of the form \begin{equation*}   \hat f \in {\rm argmin}_{f\in   F}\Big(\frac{1}{N}\sum_{i=1}^N\big(Y_i-f(X_i)\big)^2+\lambda \Psi(f)\Big) \end{equation*} for some well chosen regularization parameter \lambda.   We obtain bounds on the L_2 estimation error rate that depend on the complexity of the "true model" F^*:=\{f\in F: \Psi(f)\leq\Psi(f^*)\}, where f^*\in {\rm argmin}_{f\in F}\mathbb{E}(Y-f(X))^2 and the (X_i,Y_i)'s are independent and distributed as (X,Y). Our estimate holds under weak stochastic assumptions -- one of which being a small-ball condition satisfied by F -- and for rather flexible choices of regularization functions \Psi(\cdot). Moreover, the result holds in the learning theory framework: we do not assume any a-priori connection between the output Y and the input X.   As a proof of concept, we apply our general estimation bound to various choices of \Psi, for example, the \ell_p and S_p-norms (for p\geq1), weak-\ell_p, atomic norms, max-norm and SLOPE. In many cases, the estimation rate almost coincides with the minimax rate in the class F^*.