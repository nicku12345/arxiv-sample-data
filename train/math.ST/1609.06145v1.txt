Heteroskedasticity is a statistical anomaly that describes differing variances of error terms in a time series dataset. The presence of heteroskedasticity in data imposes serious challenges for forecasting models and many statistical tests are not valid in the presence of heteroskedasticity. Heteroskedasticity of the data affects the relation between the predictor variable and the outcome, which leads to false positive and false negative decisions in the hypothesis testing. Available approaches to study heteroskedasticity thus far adopt the strategy of accommodating heteroskedasticity in the time series and consider it an inevitable source of noise. In these existing approaches, two forecasting models are prepared for normal and heteroskedastic scenarios and a statistical test is to determine whether or not the data is heteroskedastic.   This work-in-progress research introduces a quantifying measurement for heteroskedasticity. The idea behind the proposed metric is the fact that a heteroskedastic time series features a uniformly distributed local variances. The proposed measurement is obtained by calculating the local variances using linear time invariant filters. A probability density function of the calculated local variances is then derived and compared to a uniform distribution of theoretical ultimate heteroskedasticity using statistical divergence measurements. The results demonstrated on synthetic datasets shows a strong correlation between the proposed metric and number of variances locally estimated in a heteroskedastic time series.