We develop non-asymptotically justified methods for hypothesis testing about the p-dimensional coefficients \theta^{*} in (possibly nonlinear) regression models. Given a function h:\,\mathbb{R}^{p}\mapsto\mathbb{R}^{m}, we consider the null hypothesis H_{0}:\,h(\theta^{*})\in\Omega against the alternative hypothesis H_{1}:\,h(\theta^{*})\notin\Omega, where \Omega is a nonempty closed subset of \mathbb{R}^{m} and h can be nonlinear in \theta^{*}. Our (nonasymptotic) control on the Type I and Type II errors holds for fixed n and does not rely on well-behaved estimation error or prediction error; in particular, when the number of restrictions in H_{0} is large relative to p-n, we show it is possible to bypass the sparsity assumption on \theta^{*} (for both Type I and Type II error control), regularization on the estimates of \theta^{*}, and other inherent challenges in an inverse problem. We also demonstrate an interesting link between our framework and Farkas' lemma (in math programming) under uncertainty, which points to some potential applications of our method outside traditional hypothesis testing.